{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db0dddbc",
   "metadata": {},
   "source": [
    "Q.1) Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He                    initialization?\n",
    "\n",
    "  :- The weights attached to the same neuron, continue to remain the same throughout the training. It makes the hidden units        symmetric and this problem is known as the symmetry problem. Hence to break this symmetry the weights connected to the          same neuron should not be initialized to the same value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e971e1",
   "metadata": {},
   "source": [
    "Q.2) Is it OK to initialize the bias terms to 0?\n",
    "\n",
    "  :- Thus initialized weights with zero make your network no better than a linear model. It is important to note that setting        biases to 0 will not create any problems as non-zero weights take care of breaking the symmetry and even if bias is 0, the      values in every neuron will still be different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1588fc",
   "metadata": {},
   "source": [
    "Q.3) Name three advantages of the SELU activation function over ReLU.\n",
    "\n",
    "  :- Like ReLU, SELU does not have vanishing gradient problem and hence, is used in deep neural networks. Compared to ReLUs,        SELUs cannot die. SELUs learn faster and better than other activation functions without needing further procession.\n",
    "     SELU is a self-normalizing activation function. It is a variant of the ELU . The main advantage of SELU is that we can be      sure that the output will always be standardized due to its self-normalizing behavior. That means there is no need to          include Batch-Normalization layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28afc0a3",
   "metadata": {},
   "source": [
    "Q.4) In which cases would you want to use each of the following activation functions: SELU, leaky,ReLU (and its variants),ReLU,      tanh, logistic, and softmax?\n",
    "\n",
    "  :- Sigmoid function: It maps any input value to a value between 0 and 1. It is often used in the output layer of a binary          classification problem where the model needs to output a probability value between 0 and 1.\n",
    "\n",
    "     ReLU (Rectified Linear Unit): It is one of the most popular activation functions and is used in most neural network            architectures. It outputs the input value if it is positive and 0 otherwise. It is simple, computationally efficient, and      helps in avoiding the vanishing gradient problem.\n",
    "\n",
    "     Softmax: It is a popular activation function used in the output layer of a multi-class classification problem. It maps the      input values to a probability distribution over multiple classes.\n",
    "     \n",
    "     Tanh (Hyperbolic Tangent): It maps the input value to a value between -1 and 1. It is used in the hidden layers of a            neural network and helps in avoiding the vanishing gradient problem.\n",
    "\n",
    "     Leaky ReLU: It is a variant of the ReLU activation function and outputs the input value if it is positive and a small          negative value otherwise. It helps in avoiding the dying ReLU problem and improves the training of deep neural networks.\n",
    "\n",
    "     ELU (Exponential Linear Unit): It is similar to the Leaky ReLU and outputs the input value if it is positive and an            exponential function of the input value minus 1 if it is negative. It helps in avoiding the dying ReLU problem and              improves the training of deep neural networks.\n",
    "\n",
    "     Swish: It is a recently proposed activation function that is similar to the sigmoid function. It is computationally            efficient and helps in improving the performance of deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27429a0",
   "metadata": {},
   "source": [
    "Q.5) What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?\n",
    "\n",
    "  :- The momentum hyperparameter in stochastic gradient descent (SGD) determines the contribution of the previous update to the      current update of the model parameters. When momentum is set too close to 1, e.g., 0.99999, it means that the algorithm is      giving a lot of weightage to the previous update, and the current update is making only a small contribution. This can          cause the following issues:\n",
    "\n",
    "     The algorithm can overshoot the minimum point: A high momentum value causes the algorithm to overshoot the minimum point        by carrying too much velocity from previous updates. This can cause the algorithm to oscillate around the minimum point        and fail to converge.\n",
    "\n",
    "     The learning rate effectively reduces: Since the momentum term is taking up most of the update direction, the learning          rate effectively reduces. This can slow down the learning process and take more iterations to converge. The algorithm can      get stuck in a local minimum: When the momentum is too high, the algorithm can get stuck in a local minimum and fail to        find the global minimum. This is because the momentum term can cause the algorithm to overshoot the global minimum and get      trapped in a local minimum.\n",
    "     Therefore, it is crucial to choose an appropriate value for the momentum hyperparameter. A common value for momentum is        0.9, but this value can be adjusted based on the specific problem and architecture of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20a630b",
   "metadata": {},
   "source": [
    "Q.6) Name three ways you can produce a sparse model.\n",
    "\n",
    "  :- A sparse model is a model that has most of its weights set to zero, resulting in a smaller model size and faster                computation. Here are three ways to produce a sparse model:\n",
    "\n",
    "     L1 regularization: L1 regularization is a technique that adds a penalty term to the loss function based on the absolute        values of the weights. The penalty term encourages the model to learn sparse weights, as it tries to minimize the number        of non-zero weights while still maintaining good accuracy.\n",
    "\n",
    "     Dropout: Dropout is a technique where random neurons or weights are temporarily removed during training. This technique        forces the remaining neurons to learn more robust features and reduces the dependence on specific weights, which can lead      to sparser models.\n",
    "\n",
    "     Pruning: Pruning is a technique that involves removing the weights with small magnitudes from the model. The idea behind        pruning is to reduce the number of parameters in the model without significantly affecting the accuracy. Pruning can be       done during training or after training, and there are various pruning algorithms available, such as magnitude-based             pruning, weight thresholding, and sensitivity-based pruning.\n",
    "    These techniques can be used individually or in combination to produce even sparser models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded2256b",
   "metadata": {},
   "source": [
    "Q.7) Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC        Dropout?\n",
    "\n",
    "  :- Logically, by omitting at each iteration neurons with a dropout, those omitted on an iteration are not updated during the      backpropagation. They do not exist. So the training phase is slowed down. No dropout not slow down inferences. MC-Dropout      model performs the prediction of the class for a new input differently from a classic model of deep learning. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
